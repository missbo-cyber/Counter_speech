# -*- coding: utf-8 -*-
"""Copie de Zero_shot_plus_metrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YocGciBlU9iYJxyEUpzwsk5IkQvT4RwD

Fine-tuninng LLAMA
"""

!pip install peft
!pip install accelerate
!pip install bitsandBytes #quantizition
!pip install transformers
!pip install datasets
!pip install torch
!pip install GPUtil
!pip install -U transformers

import torch
import GPUtil
import os
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
from datasets import load_dataset
from huggingface_hub import notebook_login

GPUtil.showUtilization()

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

if torch.cuda.is_available():
    print("GPU is available")
else:
    print("GPU is not available, using CPU instead")

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

if "COLAB_GPU" in os.environ:
  from google.colab import output
  output.enable_custom_widget_manager()

"""Loging to HuggingFace with our token - we need to have an access to certain models"""

from huggingface_hub import login

login("Your token here")

"""Machines do not understand language as we do - they understand numbers, therefore we need to use tokenization. There are three types of tokenization: word-based, character-based and subword-based."""

base_model_id = "meta-llama/Llama-2-7b-hf"
bnb_config = BitsAndBytesConfig( # we use this to train 7 billions of parameters having limited memory
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=False)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})

model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)
model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

"""LORA - is a small version of a language model that is adapted for a specific task. It stands for "Low-Rank Adaptation". It reduces the resourcers and makes training faster."""

config = LoraConfig(
    r=8,
    lora_alpha=64,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    bias="none",
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, config)
model.to(device)
model.config.use_cache = False

"""Loading dataset directly from hugging-face"""

ds = load_dataset("LanD-FBK/ML_MTCONAN_KN")

ds

ds_filtered = DatasetDict({
    "train": ds["train"].filter(lambda x: x["LANG"] == "ES"),
    "validation": ds["validation"].filter(lambda x: x["LANG"] == "ES")
})

ds_filtered

MAX_LEN = 256

def tokenize_concat(example):
    inp = example["input_text"]
    tgt = example["target_text"]

    inp_ids = tokenizer(inp, truncation=True, max_length=MAX_LEN // 2).input_ids
    tgt_ids = tokenizer(tgt, truncation=True, max_length=MAX_LEN - len(inp_ids)).input_ids

    input_ids = inp_ids + tgt_ids
    attention_mask = [1] * len(input_ids)
    labels = [-100] * len(inp_ids) + tgt_ids

    # padding
    pad_len = MAX_LEN - len(input_ids)
    if pad_len > 0:
        input_ids += [tokenizer.pad_token_id] * pad_len
        attention_mask += [0] * pad_len
        labels += [-100] * pad_len
    else:
        input_ids = input_ids[:MAX_LEN]
        attention_mask = attention_mask[:MAX_LEN]
        labels = labels[:MAX_LEN]

    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

def tokenize_concat_batched(batch):
    input_texts = list(batch["input_text"])
    target_texts = list(batch["target_text"])

    outs = {"input_ids": [], "attention_mask": [], "labels": []}
    for inp, tgt in zip(input_texts, target_texts):
        ex = tokenize_concat({"input_text": inp, "target_text": tgt})
        outs["input_ids"].append(ex["input_ids"])
        outs["attention_mask"].append(ex["attention_mask"])
        outs["labels"].append(ex["labels"])
    return outs

def format_example(example):
    prompt = f"HS: {example['HS']}\nKN: {example['KN']}"
    target = example['KN_CN']
    return {"input_text": prompt, "target_text": target}

ds_filtered = ds_filtered.map(format_example)



from datasets import DatasetDict

tokenized_ds = DatasetDict({
    "train": ds_filtered["train"].map(tokenize_concat_batched, batched=True, remove_columns=ds_filtered["train"].column_names),
    "validation": ds_filtered["validation"].map(tokenize_concat_batched, batched=True, remove_columns=ds_filtered["validation"].column_names)
})

print(tokenized_ds['train'][0])
print(tokenized_ds['validation'][0])

from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer)

"""Training process"""

from transformers import Trainer, TrainingArguments, EarlyStoppingCallback

training_args = TrainingArguments(
    output_dir="./model_lora_1600",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=8,
    learning_rate=1e-4,
    weight_decay=0.0,
    warmup_ratio=0.05,
    lr_scheduler_type="cosine",

    logging_steps=20,
    save_strategy="epoch",
    eval_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    max_grad_norm=0.3,
    report_to="tensorboard",
    logging_dir="./logs",
)
early_stopping = EarlyStoppingCallback(
    early_stopping_patience=1
)
trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_ds["train"], eval_dataset=tokenized_ds["validation"], data_collator=data_collator, callbacks=[early_stopping] )
trainer.train()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir ./logs

"""Showing some generated answers"""

def generate_and_show(n=5, max_new_tokens=128):
    model.eval()
    model.config.use_cache = True

    for i in range(n):
        ex = ds_filtered["validation"][i]


        prompt = f"HS: {ex['HS']}\nKN: {ex['KN']}\nGenerate CN:"


        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024,
            padding="longest"
        ).to(device)

        with torch.no_grad():
            out = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )


        decoded = tokenizer.decode(out[0], skip_special_tokens=True)


        if "Generate CN:" in decoded:
            generated = decoded.split("Generate CN:")[-1].strip()
        else:

            generated = decoded.replace(ex['HS'], '').replace(ex['KN'], '').strip()[:500]

        print("\n--- Example", i+1, "---")
        print("Input (HS):", ex["HS"], ex['KN'])
        print("Target (CN):", ex["KN_CN"])
        print("Generated (CN):", generated)

generate_and_show(n=5, max_new_tokens=128)

"""Blue - evaluates how closely the output resembles the Ground Truth"""

def generate_cn(example, max_new_tokens=128):
    model.eval()

    prompt = f"HS: {example['HS']}\nKN: {example['KN']}\nGenerate CN:"

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding="longest"
    ).to(device)

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            temperature=1.0,
            top_p=1.0,
            pad_token_id=tokenizer.eos_token_id,
        )

    decoded = tokenizer.decode(out[0], skip_special_tokens=True)


    if "Generate CN:" in decoded:
        return decoded.split("Generate CN:")[-1].strip()
    return decoded.strip()

predictions = []
references = []
number_of_examples = 0
for ex in ds_filtered["validation"]:
    generated = generate_cn(ex)
    predictions.append(generated)
    references.append(ex["KN_CN"])
    if number_of_examples == 10:
      print("Next 10 examples are generated")
      number_of_examples = 0
    number_of_examples += 1

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def compute_bleu(predictions, references):
    smoothie = SmoothingFunction().method4
    scores = []
    for pred, ref in zip(predictions, references):
        score = sentence_bleu(
            [ref.split()],
            pred.split(),
            smoothing_function=smoothie
        )
        scores.append(score)
    return sum(scores) / len(scores)

bleu = compute_bleu(predictions, references)
print("Average BLEU:", bleu)

!pip install evaluate
from evaluate import load

!pip install rouge_score

rouge_metric = load("rouge")

rouge_result = rouge_metric.compute(predictions=predictions, references=references)
rouge_l = rouge_result["rougeL"]

print("ROUGE-L:", rouge_l)

!pip install bert_score

bertscore_metric = load("bertscore")

bert_result = bertscore_metric.compute(predictions=predictions,
                                       references=references,
                                       lang="en")
bert_f1 = sum(bert_result["f1"]) / len(bert_result["f1"])

print("BERTScore F1:", bert_f1)
